
# 📝 Abstractive Text Summarization using BART

This project demonstrates how to perform **abstractive text summarization** using the `facebook/bart-large-cnn` model from Hugging Face Transformers on the CNN/DailyMail dataset. It includes both zero-shot summarization and fine-tuning of the model for better performance.

## 🚀 Project Overview

The goal of abstractive summarization is to generate a concise summary of a longer document that captures the key ideas using **paraphrasing**, rather than copying exact sentences from the original text.

### ✨ Key Features

- Uses the `facebook/bart-large-cnn` model for summarization
- Loads and processes the CNN/DailyMail dataset using Hugging Face `datasets`
- Demonstrates **inference without fine-tuning**
- Performs **custom fine-tuning** on a subset of the dataset
- Evaluates and saves the fine-tuned model
- Includes a custom blog summarization function

## 📦 Dependencies

Install required packages using pip:

```bash
pip install transformers datasets
```

If running on Colab, the script will automatically install `datasets`.

## 📊 Dataset

The project uses the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset (version 3.0.0), which is a benchmark dataset for summarization tasks.

- **Description**: The dataset consists of news articles (CNN) paired with multi-sentence summaries (DailyMail).
- **Training set**: 10,000 samples (subset for fast training)
- **Validation set**: 2,000 samples (subset for evaluation)
- Each sample includes:
  - `article`: Full news article text
  - `highlights`: Human-written summary

## 📂 Workflow Steps

1. **Load Dataset**:
   ```python
   dataset = load_dataset("cnn_dailymail", name="3.0.0")
   ```

2. **Zero-Shot Summarization** (no fine-tuning):
   ```python
   summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
   ```

3. **Tokenization and Preprocessing**:
   - Truncate and pad input and output
   - Replace padding tokens in target with -100 for loss masking

4. **Fine-Tuning**:
   - Use `Trainer` API from Hugging Face
   - 1 epoch training with batch size of 2
   - Save model and tokenizer to Google Drive

5. **Evaluation**:
   - Outputs validation loss

6. **Summarization Function**:
   - Loads the fine-tuned model
   - Summarizes long blog-style input using beam search and generation configs

## 🧪 Example

Summarizing a blog post on climate finance with:

```python
summary = summarize(blog_post)
print(summary)
```

## 📁 Directory Structure

```
.
├── abstractive_text_summarization.py  # Main script
└── README.md                          # This file
```

## 💾 Output

- Summary text generated by BART model
- Fine-tuned model and tokenizer saved for reuse
- Evaluation loss printed after training


